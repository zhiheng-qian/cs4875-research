{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SimpleEnsemble.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsjADG1IWNsY97c0pPz6re"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sH1ckzOtWxNw"},"source":["<h1>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles<h1/>\n","\n","Balaji Lakshminarayanan Alexander Pritzel Charles Blundell"]},{"cell_type":"markdown","metadata":{"id":"WazVtZ1MagbT"},"source":["<h2>Classification with Wide ResNet and CIFAR10<h2/>"]},{"cell_type":"code","metadata":{"id":"NHtidof0Zxv_","executionInfo":{"status":"ok","timestamp":1609687622142,"user_tz":-120,"elapsed":657,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}}},"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","m = nn.Softplus()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zb-YVXJ_au9e","executionInfo":{"status":"ok","timestamp":1609685579507,"user_tz":-120,"elapsed":20881,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"25f22874-0e39-49a3-d19e-5fb122dcdaf6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CVf-V55ugfUu","executionInfo":{"status":"ok","timestamp":1609686025916,"user_tz":-120,"elapsed":725,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}}},"source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate, stride=1):\n","        \"\"\"\n","        Args:\n","          in_channels:  Number of input channels.\n","          out_channels: Number of output channels.\n","          dropout_rate:  Dropout Rate\n","          stride:       Controls the stride.\n","        \"\"\"\n","        super(Block, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.BatchNorm2d(in_channels),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, bias=True, padding = 1),\n","            nn.Dropout(p = dropout_rate),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=True, stride = stride, padding = 1)\n","        )\n","        self.skip = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.skip = nn.Sequential(\n","               nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n","            )\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out += self.skip(x)\n","        return out\n","\n","class GroupOfBlocks(nn.Module):\n","    def __init__(self, in_channels, out_channels, n_blocks, dropout_rate, stride=1):\n","        super(GroupOfBlocks, self).__init__()\n","        strides = [stride] + [1]*(int(n_blocks) - 1)\n","        self.in_channels = in_channels\n","        group = []\n","\n","        for stride in strides:\n","            group.append(Block(self.in_channels, out_channels, dropout_rate, stride))\n","            self.in_channels = out_channels\n","\n","        self.group = nn.Sequential(*group)\n","\n","        # first_block = Block(in_channels, out_channels, stride)\n","        # other_blocks = [Block(out_channels, out_channels) for _ in range(1, n_blocks)]\n","        # self.group = nn.Sequential(first_block, *other_blocks)\n","\n","    def forward(self, x):\n","        return self.group(x)\n","\n","class WideResNet(nn.Module):\n","    def __init__(self, depth, widen_factor, dropout_rate, num_classes=10):\n","        super(WideResNet, self).__init__()\n","        assert ((depth-4)%6 == 0), \"Depth should be 6n+4.\"\n","        n = (depth - 4)/6\n","        k = widen_factor\n","        nStages = [16, 16*k, 32*k, 64*k]\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=nStages[0], kernel_size=3, stride=1, padding=1, bias=True)\n","        self.group1 = GroupOfBlocks(nStages[0], nStages[1], n, dropout_rate)\n","        self.group2 = GroupOfBlocks(nStages[1], nStages[2], n, dropout_rate, stride=2)\n","        self.group3 = GroupOfBlocks(nStages[2], nStages[3], n, dropout_rate, stride=2)\n","        self.bn1 = nn.BatchNorm2d(nStages[3])\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc = nn.Linear(nStages[3], num_classes)\n","        self.nStage3 = nStages[3]\n","\n","        # Initialize weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, np.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","\n","        x = self.group1(x)\n","        x = self.group2(x)\n","        x = self.group3(x)\n","        x = self.relu(self.bn1(x))\n","        x = F.avg_pool2d(x, 8)\n","        x = x.view(-1, self.nStage3)\n","        return self.fc(x)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2uYIkcKtfgp","executionInfo":{"status":"ok","timestamp":1609685592456,"user_tz":-120,"elapsed":9050,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"03d38c5d-fd2d-4af6-83d6-af9095dfe277"},"source":["import torchvision\n","import torchvision.transforms as transforms\n","data_dir = '/content/drive/My Drive/AALTO/cs4875-research/data/'\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Transform to tensor\n","    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjlorpxOtift","outputId":"097a3961-d7be-41de-d652-1b116c9eeca9"},"source":["device = torch.device('cuda:0')\n","loss_func = nn.CrossEntropyLoss()\n","m = nn.LogSoftmax(dim=1)\n","eps = 0.01*2 # input ranges from (-1, 1)\n","learning_rate = 0.01\n","\n","def ensembleWithAdversarial(model, optimizer):\n","  running_loss = 0.0\n","  for epoch in range(numEpochs):\n","    # model.train()\n","    for x, y in trainloader:\n","        x, y = x.to(device), y.to(device)\n","        x = x.clone().detach().requires_grad_(True)\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = loss_func(output, y)\n","        loss.backward(retain_graph=True)\n","        x_prime = x + eps*(torch.sign(x.grad.data))\n","        optimizer.zero_grad()\n","        output_prime = model(x_prime)\n","        loss = loss_func(output, y) + loss_func(output_prime, y)\n","        loss.backward()\n","        optimizer.step()\n","    if epoch == (numEpochs-1):\n","      running_loss = loss.item()\n","    print('Loss at epoch {} is {}'.format(epoch, loss.item()))\n","  return running_loss\n","\n","\n","numEpochs = 40\n","training_loss = []\n","t0 = time.time()\n","for i in range(4):\n","  model = WideResNet(28, 10, 0.5)\n","  model.to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  training_loss.append(ensembleWithAdversarial(model, optimizer))\n","print('NLL Loss is {}'.format(np.mean(training_loss)))\n","print('Training time: {} seconds'.format(time.time() - t0))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss at epoch 0 is 2.8912243843078613\n","Loss at epoch 1 is 3.298201084136963\n","Loss at epoch 2 is 2.4600324630737305\n","Loss at epoch 3 is 2.1528806686401367\n","Loss at epoch 4 is 1.593316674232483\n","Loss at epoch 5 is 1.1246696710586548\n","Loss at epoch 6 is 1.7617130279541016\n","Loss at epoch 7 is 1.0838863849639893\n","Loss at epoch 8 is 1.858198642730713\n","Loss at epoch 9 is 1.5756192207336426\n","Loss at epoch 10 is 1.4269468784332275\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dLbQRtqCua6l"},"source":[""],"execution_count":null,"outputs":[]}]}