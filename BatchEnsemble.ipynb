{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BatchEnsemble.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5V/hxHEp+dDVUp/q5wulY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1fJXab4ybGGq"},"source":["<h1>BatchEnsemble: An Alternative Approach To Efficient Ensemble and Lifelong Learning<h1/>\n","\n","Yeming Wen, Dustin Tran & Jimmy Ba"]},{"cell_type":"markdown","metadata":{"id":"APXrAy0lba_B"},"source":["<h2>Classification with Wide ResNet and CIFAR10<h2/>"]},{"cell_type":"code","metadata":{"id":"cF4t-QLebjab"},"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","m = nn.Softplus()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P1C0MsYQbjW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615392569511,"user_tz":-120,"elapsed":23068,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"ff5d7c88-6409-4444-889c-61b1b63d1ab3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OXWXi-TqbjUC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610297423225,"user_tz":-120,"elapsed":30654,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"3c908b8a-79af-4fec-f406-8716974f202b"},"source":["import torchvision\n","import torchvision.transforms as transforms\n","data_dir = '/content/drive/My Drive/AALTO/cs4875-research/data/'\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Transform to tensor\n","    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WmrxhBVSIG_T"},"source":["class Cov2dEnsemble(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, num_models=4, first_layer=False):\n","      super(Cov2dEnsemble, self).__init__()\n","      self.in_channels = in_channels\n","      self.out_channels = out_channels\n","      self.num_models = num_models\n","      self.first_layer = first_layer\n","      self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n","      self.alpha = nn.Parameter(torch.Tensor(num_models, in_channels))\n","      self.gamma = nn.Parameter(torch.Tensor(num_models, out_channels))\n","      nn.init.normal_(self.alpha, mean=1., std=0.5)\n","      nn.init.normal_(self.gamma, mean=1., std=0.5)\n","\n","    def forward(self, x):\n","      if not self.training and self.first_layer:\n","        x = torch.cat([x for i in range(self.num_models)], dim=0)\n","      examples_per_model = int(x.size(0) / self.num_models)\n","      alpha = torch.cat([self.alpha for i in range(examples_per_model)], dim=1).view([-1, self.in_channels])\n","      alpha.unsqueeze_(-1).unsqueeze_(-1)\n","      gamma = torch.cat([self.gamma for i in range(examples_per_model)], dim=1).view([-1, self.out_channels])\n","      gamma.unsqueeze_(-1).unsqueeze_(-1)\n","      if extra != 0:\n","        alpha = torch.cat([alpha, alpha[:extra]], dim=0)\n","        gamma = torch.cat([gamma, gamma[:extra]], dim=0)\n","      return self.conv1(x*alpha)*gamma\n","\n","class DenseEnsemble(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, num_models=4):\n","      super(DenseEnsemble, self).__init__()\n","      self.in_channels = in_channels\n","      self.out_channels = out_channels\n","      self.num_models = num_models\n","      self.fc = nn.Linear(in_channels, out_channels, bias=False)\n","      self.alpha = nn.Parameter(torch.Tensor(num_models, in_channels))\n","      self.gamma = nn.Parameter(torch.Tensor(num_models, out_channels))\n","      nn.init.normal_(self.alpha, mean=1., std=0.5)\n","      nn.init.normal_(self.gamma, mean=1., std=0.5)\n","\n","    def forward(self, x):\n","      examples_per_model = int(x.size(0) / self.num_models)\n","      alpha = torch.cat([self.alpha for i in range(examples_per_model)], dim=1).view([-1, self.in_channels])\n","      gamma = torch.cat([self.gamma for i in range(examples_per_model)], dim=1).view([-1, self.out_channels])\n","      if extra != 0:\n","        alpha = torch.cat([alpha, alpha[:extra]], dim=0)\n","        gamma = torch.cat([gamma, gamma[:extra]], dim=0)\n","      return self.fc(x*alpha)*gamma\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBffc7VwbjPb"},"source":["class BlockBatchEnsemble(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate, stride=1, num_models=4):\n","        super(BlockBatchEnsemble, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.conv1 = Cov2dEnsemble(in_channels, out_channels, 3, stride=1, padding=1, num_models=num_models)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = Cov2dEnsemble(out_channels, out_channels, 3, stride=stride, padding=1, num_models=num_models)\n","        self.num_models = num_models\n","        self.skip = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.skip = nn.Sequential(\n","                Cov2dEnsemble(in_channels, out_channels, 1, stride=stride, padding=0, num_models=num_models),\n","            )\n","\n","    def forward(self, x):\n","        curr_bs = x.size(0)\n","        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += self.skip(x)\n","        return out\n","\n","class GroupBlockBatchEnsemble(nn.Module):\n","    def __init__(self, in_channels, out_channels, n_blocks, dropout_rate, stride=1, num_models=4):\n","        super(GroupBlockBatchEnsemble, self).__init__()\n","        strides = [stride] + [1]*(int(n_blocks) - 1)\n","        self.in_channels = in_channels\n","        group = []\n","\n","        for stride in strides:\n","            group.append(BlockBatchEnsemble(self.in_channels, out_channels, dropout_rate, stride))\n","            self.in_channels = out_channels\n","\n","        self.group = nn.Sequential(*group)\n","\n","    def forward(self, x):\n","        return self.group(x)\n","\n","class WideResNetBatchEnsemble(nn.Module):\n","    def __init__(self, depth, widen_factor, dropout_rate, num_classes=10, num_models=4):\n","        super(WideResNetBatchEnsemble, self).__init__()\n","        assert ((depth-4)%6 == 0), \"Depth should be 6n+4.\"\n","        n = (depth - 4)/6\n","        k = widen_factor\n","        nStages = [16, 16*k, 32*k, 64*k]\n","        self.num_models = num_models\n","        self.num_classes = num_classes\n","\n","        self.conv1 = Cov2dEnsemble(in_channels=3, out_channels=nStages[0], kernel_size=1, stride=1, padding=0, num_models=num_models, first_layer=True)\n","        self.group1 = GroupBlockBatchEnsemble(nStages[0], nStages[1], n, dropout_rate, stride=1, num_models=num_models)\n","        self.group2 = GroupBlockBatchEnsemble(nStages[1], nStages[2], n, dropout_rate, stride=2, num_models=num_models)\n","        self.group3 = GroupBlockBatchEnsemble(nStages[2], nStages[3], n, dropout_rate, stride=2, num_models=num_models)\n","        self.bn1 = nn.BatchNorm2d(nStages[3])\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc = DenseEnsemble(nStages[3], num_classes, num_models)\n","        self.nStage3 = nStages[3]\n","\n","        # Initialize weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, np.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.group1(x)\n","        x = self.group2(x)\n","        x = self.group3(x)\n","        x = self.relu(self.bn1(x))\n","        x = F.avg_pool2d(x, 8)\n","        x = x.view(-1, self.nStage3)\n","        x = self.fc(x)\n","        if not self.training:\n","            x=F.softmax(x, dim=1)\n","            return x.view([self.num_models, -1, self.num_classes]).mean(dim=0)\n","        return x\n","\n","\n","def compute_accuracy(net, testloader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return correct / total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAGSpwbybjMn"},"source":["device = torch.device('cuda:0')\n","loss_func = nn.CrossEntropyLoss()\n","m = nn.LogSoftmax(dim=1)\n","learning_rate = 0.01\n","\n","def compute_brier_score(p, y):\n","  brier_score = torch.mean((y-torch.argmax(p, 1).float())**2)\n","  return brier_score\n","\n","def ensembleInBatch(model, optimizer):\n","  running_loss = 0.0\n","  running_brier = 0.0\n","  model.train()\n","  for epoch in range(numEpochs):\n","    brier_score = 0.0\n","    total = 0\n","    for x, y in trainloader:\n","        x, y = x.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        output = model(x)\n","        batch_brier_score = compute_brier_score(output, y)\n","        brier_score += torch.sum(batch_brier_score, 0).cpu().numpy().item()\n","        loss = loss_func(output, y)\n","        loss.backward()\n","        optimizer.step()\n","        total += y.size(0)\n","    if epoch == (numEpochs-1):\n","      running_loss = loss.item()\n","    print('Loss at epoch {} is {}'.format(epoch, loss.item()))\n","    print('Brier score at epoch {} is {}'.format(epoch, brier_score/total))\n","  return running_loss, brier_score/total\n","\n","\n","numEpochs = 40\n","t0 = time.time()\n","model = WideResNetBatchEnsemble(28, 4, 0.5)\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","loss, brier = ensembleInBatch(model, optimizer)\n","time_one = time.time() - t0\n","accuracy = compute_accuracy(model, testloader)\n","\n","print('Accuracy of the network on the test images: %.3f' % accuracy)\n","print('NLL Loss is {}'.format(loss))\n","print('Brier score is {}'.format(brier))\n","print('Training time: {} seconds'.format(time_one))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Hh9bgg_LohS"},"source":["torch.save(model.state_dict(), '/content/drive/My Drive/AALTO/cs4875-research/archive/batch_ensemble.pth')\n","print('Model saved to %s.' % ('batch_ensemble.pth'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n_L-vXXHcifh"},"source":["<h2>Time series prediction with MIMIC3 and LSTM<h2/>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOP_o2ZsHYym","executionInfo":{"status":"ok","timestamp":1615392588799,"user_tz":-120,"elapsed":6942,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"bd3f2ead-f08d-4b02-fe95-364cede72aef"},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from mydatasets import calculate_num_features, VisitSequenceWithLabelDataset, visit_collate_fn\n","# !pip3 install pickle5\n","import pickle5 as pickle\n","from torch.utils.data import DataLoader\n","import torch.optim as optim"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pickle5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n","\r\u001b[K     |██▌                             | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.3MB/s \n","\u001b[?25hBuilding wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219252 sha256=2715c1ac9ffeed15ad3da45cc79e314a9971e5ee5c75c289808c03a44007f3c4\n","  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mhJHP95pHZpo"},"source":["# Data preprocessing and training process refers to https://github.com/jiaweizhu830/Time-Series-Mortality-Prediction-in-ICU-via-PyTorch\n","# Train : test = 8:2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5ub6ib4Nq_d","executionInfo":{"status":"ok","timestamp":1615392622909,"user_tz":-120,"elapsed":31312,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"cecda671-0f27-4b87-d881-ca3cf012d1f3"},"source":["\n","torch.manual_seed(0)\n","if torch.cuda.is_available():\n","\ttorch.cuda.manual_seed(0)\n","\n","# Set a correct path to the data files that you preprocessed\n","PATH_TRAIN_SEQS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/train/mortality.seqs.train\"\n","PATH_TRAIN_LABELS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/train/mortality.labels.train\"\n","PATH_TEST_SEQS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/test/mortality.seqs.test\"\n","PATH_TEST_LABELS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/test/mortality.labels.test\"\n","PATH_OUTPUT = \"/content/drive/My Drive/AALTO/cs4875-research/output/\"\n","\n","NUM_EPOCHS = 1\n","BATCH_SIZE = 128\n","USE_CUDA = False  # Set 'True' if you want to use GPU\n","NUM_WORKERS = 0\n","\n","# Data loading\n","print('===> Loading entire datasets')\n","train_seqs = pickle.load(open(PATH_TRAIN_SEQS, 'rb'))\n","train_labels = pickle.load(open(PATH_TRAIN_LABELS, 'rb'))\n","test_seqs = pickle.load(open(PATH_TEST_SEQS, 'rb'))\n","test_labels = pickle.load(open(PATH_TEST_LABELS, 'rb'))\n","print('===> done Loading')\n","num_features = calculate_num_features(train_seqs)\n","print(num_features)\n","\n","train_dataset = VisitSequenceWithLabelDataset(train_seqs, train_labels, num_features)\n","test_dataset = VisitSequenceWithLabelDataset(test_seqs, test_labels, num_features)\n","print('===> done datasets')\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=visit_collate_fn, num_workers=NUM_WORKERS, drop_last=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, collate_fn=visit_collate_fn, num_workers=NUM_WORKERS, drop_last=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["===> Loading entire datasets\n","===> done Loading\n","5067\n","===> done datasets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WXAEbKbwbmMk"},"source":["class AverageMeter(object):\n","\t\"\"\"Computes and stores the average and current value\"\"\"\n","\n","\tdef __init__(self):\n","\t\tself.reset()\n","\n","\tdef reset(self):\n","\t\tself.val = 0\n","\t\tself.avg = 0\n","\t\tself.sum = 0\n","\t\tself.count = 0\n","\n","\tdef update(self, val, n=1):\n","\t\tself.val = val\n","\t\tself.sum += val * n\n","\t\tself.count += n\n","\t\tself.avg = self.sum / self.count\n","\n","def compute_brier_score(p, y):\n","  brier_score = torch.mean((y-torch.argmax(p, 1).float())**2)\n","  return brier_score\n","\n","def compute_batch_accuracy(output, target):\n","\t\"\"\"Computes the accuracy for a batch\"\"\"\n","\twith torch.no_grad():\n","\n","\t\tbatch_size = target.size(0)\n","\t\t_, pred = output.max(1)\n","\t\tcorrect = pred.eq(target).sum()\n","\n","\t\treturn correct * 100.0 / batch_size\n","\n","def train(model, device, data_loader, criterion, optimizer, epoch, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tdata_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\n","\tmodel.train()\n","\n","\tend = time.time()\n","\tfor i, (input, target) in enumerate(data_loader):\n","\t\t# measure data loading time\n","\t\tdata_time.update(time.time() - end)\n","\n","\t\tseqs, lengths = input\n","\t\tseqs = seqs.to(device)\n","\t\ttarget = target.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\toutput = model(seqs, lengths)\n","\t\tloss = criterion(output, target)\n","\t\tassert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n","\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\n","\t\t# measure elapsed time\n","\t\tbatch_time.update(time.time() - end)\n","\t\tend = time.time()\n","\n","\t\tlosses.update(loss.item(), target.size(0))\n","\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\tif i % print_freq == 0:\n","\t\t\tprint('Epoch: [{0}][{1}/{2}]\\t'\n","\t\t\t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t\t\t  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","\t\t\t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t\t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t\t\tepoch, i, len(data_loader), batch_time=batch_time,\n","\t\t\t\tdata_time=data_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg\n","\n","def advTrain(model, device, data_loader, criterion, optimizer, epoch, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tdata_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\teps = 5067*0.01\n","\ttotal = 0\n","\tbrier_score = 0.0\n","\n","\tmodel.train()\n","\n","\tend = time.time()\n","\tfor i, (input, target) in enumerate(data_loader):\n","\t\t# measure data loading time\n","\t\tdata_time.update(time.time() - end)\n","\t\tseqs, lengths = input\n","\t\tseqs = seqs.to(device)\n","\t\tseqs = seqs.clone().detach().requires_grad_(True)\n","    \n","\t\ttarget = target.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\toutput = model(seqs, lengths)\n","\t\tbatch_brier_score = compute_brier_score(output, target)\n","\t\tbrier_score+= torch.sum(batch_brier_score, 0).cpu().numpy().item()\n","\t\ttotal += target.size(0)\n","\t\tloss = criterion(output, target)\n","\t\tassert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n","\n","\t\tloss.backward(retain_graph=True)\n","\n","\t\tseqs_prime = seqs + eps*(torch.sign(seqs.grad.data))\n","    \n","\t\toptimizer.zero_grad()\n","\t\toutput_prime = model(seqs_prime, lengths)\n","\t\tloss = criterion(output, target) + criterion(output_prime, target)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\n","\t\t# measure elapsed time\n","\t\tbatch_time.update(time.time() - end)\n","\t\tend = time.time()\n","\n","\t\tlosses.update(loss.item(), target.size(0))\n","\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\tif i % print_freq == 0:\n","\t\t\tprint('Epoch: [{0}][{1}/{2}]\\t'\n","\t\t\t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t\t\t  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","\t\t\t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t\t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t\t\tepoch, i, len(data_loader), batch_time=batch_time,\n","\t\t\t\tdata_time=data_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, brier_score/total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ws4WnGhF7L0W"},"source":["class DenseEnsemble(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, padding=0, num_models=4, first_layer=False):\n","      super(DenseEnsemble, self).__init__()\n","      self.in_channels = in_channels\n","      self.out_channels = out_channels\n","      self.num_models = num_models\n","      self.first_layer = first_layer\n","      self.fc = nn.Linear(in_channels, out_channels, bias=False)\n","      self.alpha = nn.Parameter(torch.Tensor(num_models, in_channels))\n","      self.gamma = nn.Parameter(torch.Tensor(num_models, out_channels))\n","      nn.init.normal_(self.alpha, mean=1., std=0.5)\n","      nn.init.normal_(self.gamma, mean=1., std=0.5)\n","\n","    def forward(self, x):\n","      examples_per_model = int(x.size(0) / self.num_models)\n","      extra = x.size(0) - (examples_per_model * self.num_models)\n","      if self.first_layer:\n","        alpha = torch.cat([self.alpha for i in range(examples_per_model)], dim=1).view([x.size(0), -1, self.in_channels])\n","        gamma = torch.cat([self.gamma for i in range(examples_per_model)], dim=1).view([x.size(0), -1, self.out_channels])\n","      if not self.first_layer:\n","        alpha = torch.cat([self.alpha for i in range(examples_per_model)], dim=1).view([-1, self.in_channels])\n","        gamma = torch.cat([self.gamma for i in range(examples_per_model)], dim=1).view([-1, self.out_channels])\n","      if extra != 0:\n","        alpha = torch.cat([alpha, alpha[:extra]], dim=0)\n","        gamma = torch.cat([gamma, gamma[:extra]], dim=0)\n","      return self.fc(x*alpha)*gamma\n","\n","class LSTMEnsemble(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_layers = 1, stride=1, padding=0, num_models=4, dropout = 0.1, batch_first = True):\n","      super(LSTMEnsemble, self).__init__()\n","      self.in_channels = in_channels\n","      self.out_channels = out_channels\n","      self.num_models = num_models\n","      self.lstm = nn.LSTM(input_size = in_channels, hidden_size = out_channels, num_layers = 1, dropout = dropout, batch_first = True)\n","      self.alpha = nn.Parameter(torch.Tensor(num_models, in_channels))\n","      self.gamma = nn.Parameter(torch.Tensor(num_models, out_channels))\n","      nn.init.normal_(self.alpha, mean=1., std=0.5)\n","      nn.init.normal_(self.gamma, mean=1., std=0.5)\n","\n","    def forward(self, x, lengths, seq_len):\n","      examples_per_model = int(x.size(0) / self.num_models)\n","      extra = x.size(0) - (examples_per_model * self.num_models)\n","      alpha = torch.cat([self.alpha for i in range(examples_per_model)], dim=1).view([x.size(0), -1, self.in_channels])\n","      gamma = torch.cat([self.gamma for i in range(examples_per_model)], dim=1).view([x.size(0), -1, self.out_channels])\n","      if extra != 0:\n","        alpha = torch.cat([alpha, alpha[:extra]], dim=0)\n","        gamma = torch.cat([gamma, gamma[:extra]], dim=0)\n","      input = x*alpha\n","      x = pack_padded_sequence(input, lengths, batch_first = True)\n","      x, _ = self.lstm(x)\n","      x, _ = pad_packed_sequence(x, batch_first = True, total_length = seq_len)\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2lrxqDBhHc0P"},"source":["class MyLSTMEnsemble(nn.Module):\n","    def __init__(self, dim_input, dropout_rate=0.1, num_models=4):\n","        super(MyLSTMEnsemble, self).__init__()\n","        self.num_models = num_models\n","        self.dim_input = dim_input\n","        self.fc1 = DenseEnsemble(dim_input, 64, num_models, first_layer = True)\n","        self.lstm = LSTMEnsemble(in_channels = 64, out_channels = 64, num_layers = 1, dropout = dropout_rate, batch_first = True)\n","        self.fc2 = DenseEnsemble(64, 2, num_models)\n","    def forward(self, x, lengths):\n","      lengths = lengths.long()\n","      batch_size, seq_len, num_features = x.size()\n","      x = self.fc1(x)\n","      x = torch.sigmoid(x)\n","      x = self.lstm(x, lengths, seq_len)\n","      y = torch.zeros(batch_size, 64).float()\n","      for i in range(batch_size):\n","        y[i, :] = x[i, lengths[i]-1, :]\n","      x = self.fc2(y)\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSPpmEGsNq3q","executionInfo":{"status":"ok","timestamp":1615400547639,"user_tz":-120,"elapsed":4861944,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"53d5c24c-c7ab-4515-9a16-43a444f2fb1c"},"source":["for i in range(1, 5):\n","  model = MyLSTMEnsemble(num_features)\n","  criterion = nn.CrossEntropyLoss()\n","  NUM_EPOCHS = 20\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_CUDA else \"cpu\")\n","  model.to(device)\n","  criterion.to(device)\n","\n","  optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0004)\n","  best_val_acc = 0.0\n","  train_losses, train_accuracies = [], []\n","  valid_losses, valid_accuracies = [], []\n","  training_brier = []\n","  t0 = time.time()\n","  for epoch in range(NUM_EPOCHS):\n","      train_loss, train_accuracy, brier = advTrain(model, device, train_loader, criterion, optimizer, epoch, print_freq = len(train_loader)-1)\n","      train_losses.append(train_loss)\n","      training_brier.append(brier)\n","      train_accuracies.append(train_accuracy)\n","  time_one = time.time() - t0\n","  print('NLL Loss is {}'.format(np.mean(train_losses)))\n","  print('Brier score is {}'.format(np.mean(training_brier)))\n","  print('Training time: {} seconds'.format(time_one))\n","  test_loss, test_accuracy, test_results = evaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","  print('test accuracy {}'.format(test_accuracy))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [0][0/289]\tTime 0.127 (0.127)\tData 0.006 (0.006)\tLoss 1.5911 (1.5911)\tAccuracy 58.594 (58.594)\n","Epoch: [0][288/289]\tTime 0.154 (0.211)\tData 0.008 (0.006)\tLoss 0.7438 (0.9786)\tAccuracy 80.469 (78.549)\n","Epoch: [1][0/289]\tTime 0.123 (0.123)\tData 0.006 (0.006)\tLoss 0.7017 (0.7017)\tAccuracy 82.812 (82.812)\n","Epoch: [1][288/289]\tTime 0.492 (0.211)\tData 0.010 (0.006)\tLoss 0.3206 (0.4896)\tAccuracy 89.062 (83.410)\n","Epoch: [2][0/289]\tTime 0.157 (0.157)\tData 0.006 (0.006)\tLoss 0.3127 (0.3127)\tAccuracy 89.062 (89.062)\n","Epoch: [2][288/289]\tTime 0.341 (0.216)\tData 0.008 (0.006)\tLoss 0.3448 (0.3329)\tAccuracy 84.375 (86.097)\n","Epoch: [3][0/289]\tTime 0.207 (0.207)\tData 0.008 (0.008)\tLoss 0.3415 (0.3415)\tAccuracy 85.156 (85.156)\n","Epoch: [3][288/289]\tTime 0.160 (0.223)\tData 0.005 (0.006)\tLoss 0.2658 (0.3190)\tAccuracy 88.281 (86.821)\n","Epoch: [4][0/289]\tTime 0.156 (0.156)\tData 0.006 (0.006)\tLoss 0.2582 (0.2582)\tAccuracy 90.625 (90.625)\n","Epoch: [4][288/289]\tTime 0.170 (0.222)\tData 0.006 (0.006)\tLoss 0.3220 (0.3102)\tAccuracy 82.812 (87.165)\n","Epoch: [5][0/289]\tTime 0.145 (0.145)\tData 0.007 (0.007)\tLoss 0.3284 (0.3284)\tAccuracy 84.375 (84.375)\n","Epoch: [5][288/289]\tTime 0.213 (0.221)\tData 0.007 (0.006)\tLoss 0.2452 (0.3066)\tAccuracy 88.281 (87.276)\n","Epoch: [6][0/289]\tTime 0.215 (0.215)\tData 0.008 (0.008)\tLoss 0.2910 (0.2910)\tAccuracy 86.719 (86.719)\n","Epoch: [6][288/289]\tTime 0.101 (0.213)\tData 0.004 (0.006)\tLoss 0.2492 (0.3045)\tAccuracy 89.844 (87.297)\n","Epoch: [7][0/289]\tTime 0.124 (0.124)\tData 0.005 (0.005)\tLoss 0.2684 (0.2684)\tAccuracy 89.062 (89.062)\n","Epoch: [7][288/289]\tTime 0.145 (0.207)\tData 0.005 (0.006)\tLoss 0.3364 (0.3005)\tAccuracy 85.938 (87.543)\n","Epoch: [8][0/289]\tTime 0.171 (0.171)\tData 0.007 (0.007)\tLoss 0.2654 (0.2654)\tAccuracy 89.844 (89.844)\n","Epoch: [8][288/289]\tTime 0.143 (0.206)\tData 0.005 (0.006)\tLoss 0.2150 (0.2996)\tAccuracy 91.406 (87.495)\n","Epoch: [9][0/289]\tTime 0.141 (0.141)\tData 0.006 (0.006)\tLoss 0.3171 (0.3171)\tAccuracy 83.594 (83.594)\n","Epoch: [9][288/289]\tTime 0.166 (0.205)\tData 0.005 (0.006)\tLoss 0.2397 (0.2980)\tAccuracy 90.625 (87.503)\n","Epoch: [10][0/289]\tTime 0.145 (0.145)\tData 0.006 (0.006)\tLoss 0.3120 (0.3120)\tAccuracy 85.938 (85.938)\n","Epoch: [10][288/289]\tTime 0.146 (0.206)\tData 0.005 (0.006)\tLoss 0.2697 (0.2955)\tAccuracy 89.062 (87.741)\n","Epoch: [11][0/289]\tTime 0.344 (0.344)\tData 0.009 (0.009)\tLoss 0.2344 (0.2344)\tAccuracy 92.969 (92.969)\n","Epoch: [11][288/289]\tTime 0.191 (0.205)\tData 0.006 (0.006)\tLoss 0.3023 (0.2936)\tAccuracy 87.500 (87.757)\n","Epoch: [12][0/289]\tTime 0.166 (0.166)\tData 0.008 (0.008)\tLoss 0.2477 (0.2477)\tAccuracy 89.844 (89.844)\n","Epoch: [12][288/289]\tTime 0.146 (0.204)\tData 0.005 (0.006)\tLoss 0.3402 (0.2917)\tAccuracy 85.156 (87.754)\n","Epoch: [13][0/289]\tTime 0.141 (0.141)\tData 0.006 (0.006)\tLoss 0.2583 (0.2583)\tAccuracy 86.719 (86.719)\n","Epoch: [13][288/289]\tTime 0.169 (0.203)\tData 0.005 (0.006)\tLoss 0.3338 (0.2916)\tAccuracy 83.594 (87.805)\n","Epoch: [14][0/289]\tTime 0.124 (0.124)\tData 0.006 (0.006)\tLoss 0.2767 (0.2767)\tAccuracy 87.500 (87.500)\n","Epoch: [14][288/289]\tTime 0.161 (0.202)\tData 0.005 (0.006)\tLoss 0.2158 (0.2880)\tAccuracy 90.625 (88.038)\n","Epoch: [15][0/289]\tTime 0.164 (0.164)\tData 0.007 (0.007)\tLoss 0.2665 (0.2665)\tAccuracy 86.719 (86.719)\n","Epoch: [15][288/289]\tTime 0.119 (0.203)\tData 0.006 (0.006)\tLoss 0.3132 (0.2865)\tAccuracy 89.844 (88.027)\n","Epoch: [16][0/289]\tTime 0.197 (0.197)\tData 0.007 (0.007)\tLoss 0.3281 (0.3281)\tAccuracy 88.281 (88.281)\n","Epoch: [16][288/289]\tTime 0.144 (0.202)\tData 0.005 (0.006)\tLoss 0.3630 (0.2866)\tAccuracy 87.500 (87.997)\n","Epoch: [17][0/289]\tTime 0.143 (0.143)\tData 0.006 (0.006)\tLoss 0.2642 (0.2642)\tAccuracy 86.719 (86.719)\n","Epoch: [17][288/289]\tTime 0.190 (0.204)\tData 0.005 (0.006)\tLoss 0.2539 (0.2832)\tAccuracy 87.500 (88.195)\n","Epoch: [18][0/289]\tTime 0.264 (0.264)\tData 0.008 (0.008)\tLoss 0.2804 (0.2804)\tAccuracy 88.281 (88.281)\n","Epoch: [18][288/289]\tTime 0.157 (0.208)\tData 0.006 (0.006)\tLoss 0.3289 (0.2812)\tAccuracy 87.500 (88.203)\n","Epoch: [19][0/289]\tTime 0.289 (0.289)\tData 0.009 (0.009)\tLoss 0.3342 (0.3342)\tAccuracy 87.500 (87.500)\n","Epoch: [19][288/289]\tTime 0.216 (0.205)\tData 0.006 (0.006)\tLoss 0.2974 (0.2812)\tAccuracy 90.625 (88.354)\n","NLL Loss is 0.3409256663908183\n","Brier score is 0.0010194243856779847\n","Training time: 1207.7327978610992 seconds\n","Test: [0/293]\tTime 0.005 (0.005)\tLoss 0.1740 (0.1740)\tAccuracy 96.875 (96.875)\n","Test: [292/293]\tTime 0.004 (0.004)\tLoss 0.3919 (0.2968)\tAccuracy 78.125 (86.999)\n","test accuracy 86.99872013651877\n","Epoch: [0][0/289]\tTime 0.165 (0.165)\tData 0.006 (0.006)\tLoss 1.5193 (1.5193)\tAccuracy 69.531 (69.531)\n","Epoch: [0][288/289]\tTime 0.147 (0.214)\tData 0.005 (0.006)\tLoss 0.7266 (0.9891)\tAccuracy 83.594 (78.560)\n","Epoch: [1][0/289]\tTime 0.278 (0.278)\tData 0.007 (0.007)\tLoss 0.7811 (0.7811)\tAccuracy 79.688 (79.688)\n","Epoch: [1][288/289]\tTime 0.151 (0.213)\tData 0.005 (0.006)\tLoss 0.3190 (0.4987)\tAccuracy 87.500 (82.383)\n","Epoch: [2][0/289]\tTime 0.126 (0.126)\tData 0.006 (0.006)\tLoss 0.3156 (0.3156)\tAccuracy 85.938 (85.938)\n","Epoch: [2][288/289]\tTime 0.211 (0.217)\tData 0.005 (0.006)\tLoss 0.3192 (0.3373)\tAccuracy 85.938 (86.102)\n","Epoch: [3][0/289]\tTime 0.183 (0.183)\tData 0.007 (0.007)\tLoss 0.4362 (0.4362)\tAccuracy 83.594 (83.594)\n","Epoch: [3][288/289]\tTime 0.171 (0.225)\tData 0.007 (0.006)\tLoss 0.2924 (0.3211)\tAccuracy 89.062 (86.759)\n","Epoch: [4][0/289]\tTime 0.158 (0.158)\tData 0.007 (0.007)\tLoss 0.3943 (0.3943)\tAccuracy 84.375 (84.375)\n","Epoch: [4][288/289]\tTime 0.191 (0.220)\tData 0.006 (0.006)\tLoss 0.3005 (0.3120)\tAccuracy 88.281 (87.084)\n","Epoch: [5][0/289]\tTime 0.219 (0.219)\tData 0.008 (0.008)\tLoss 0.3088 (0.3088)\tAccuracy 89.062 (89.062)\n","Epoch: [5][288/289]\tTime 0.154 (0.217)\tData 0.006 (0.006)\tLoss 0.3138 (0.3058)\tAccuracy 87.500 (87.146)\n","Epoch: [6][0/289]\tTime 0.174 (0.174)\tData 0.007 (0.007)\tLoss 0.2892 (0.2892)\tAccuracy 89.844 (89.844)\n","Epoch: [6][288/289]\tTime 0.167 (0.206)\tData 0.006 (0.006)\tLoss 0.2213 (0.3015)\tAccuracy 93.750 (87.478)\n","Epoch: [7][0/289]\tTime 0.120 (0.120)\tData 0.006 (0.006)\tLoss 0.2512 (0.2512)\tAccuracy 89.062 (89.062)\n","Epoch: [7][288/289]\tTime 0.345 (0.203)\tData 0.008 (0.006)\tLoss 0.3058 (0.3004)\tAccuracy 89.062 (87.392)\n","Epoch: [8][0/289]\tTime 0.217 (0.217)\tData 0.009 (0.009)\tLoss 0.2488 (0.2488)\tAccuracy 91.406 (91.406)\n","Epoch: [8][288/289]\tTime 0.254 (0.203)\tData 0.006 (0.006)\tLoss 0.2880 (0.2980)\tAccuracy 87.500 (87.481)\n","Epoch: [9][0/289]\tTime 0.142 (0.142)\tData 0.006 (0.006)\tLoss 0.2573 (0.2573)\tAccuracy 89.062 (89.062)\n","Epoch: [9][288/289]\tTime 0.321 (0.197)\tData 0.008 (0.006)\tLoss 0.2672 (0.2961)\tAccuracy 89.062 (87.535)\n","Epoch: [10][0/289]\tTime 0.131 (0.131)\tData 0.006 (0.006)\tLoss 0.2910 (0.2910)\tAccuracy 88.281 (88.281)\n","Epoch: [10][288/289]\tTime 0.146 (0.204)\tData 0.005 (0.006)\tLoss 0.3355 (0.2936)\tAccuracy 85.156 (87.814)\n","Epoch: [11][0/289]\tTime 0.316 (0.316)\tData 0.010 (0.010)\tLoss 0.2793 (0.2793)\tAccuracy 85.156 (85.156)\n","Epoch: [11][288/289]\tTime 0.147 (0.201)\tData 0.006 (0.006)\tLoss 0.3123 (0.2919)\tAccuracy 84.375 (87.746)\n","Epoch: [12][0/289]\tTime 0.311 (0.311)\tData 0.009 (0.009)\tLoss 0.2546 (0.2546)\tAccuracy 90.625 (90.625)\n","Epoch: [12][288/289]\tTime 0.124 (0.206)\tData 0.004 (0.006)\tLoss 0.2872 (0.2892)\tAccuracy 89.062 (87.938)\n","Epoch: [13][0/289]\tTime 0.144 (0.144)\tData 0.006 (0.006)\tLoss 0.2708 (0.2708)\tAccuracy 89.844 (89.844)\n","Epoch: [13][288/289]\tTime 0.120 (0.205)\tData 0.005 (0.006)\tLoss 0.3990 (0.2899)\tAccuracy 83.594 (88.033)\n","Epoch: [14][0/289]\tTime 0.188 (0.188)\tData 0.007 (0.007)\tLoss 0.3119 (0.3119)\tAccuracy 82.812 (82.812)\n","Epoch: [14][288/289]\tTime 0.146 (0.203)\tData 0.005 (0.006)\tLoss 0.4175 (0.2872)\tAccuracy 81.250 (87.870)\n","Epoch: [15][0/289]\tTime 0.168 (0.168)\tData 0.007 (0.007)\tLoss 0.3036 (0.3036)\tAccuracy 89.844 (89.844)\n","Epoch: [15][288/289]\tTime 0.116 (0.201)\tData 0.004 (0.006)\tLoss 0.3290 (0.2851)\tAccuracy 87.500 (88.184)\n","Epoch: [16][0/289]\tTime 0.138 (0.138)\tData 0.005 (0.005)\tLoss 0.3707 (0.3707)\tAccuracy 82.812 (82.812)\n","Epoch: [16][288/289]\tTime 0.177 (0.205)\tData 0.006 (0.006)\tLoss 0.2915 (0.2849)\tAccuracy 89.844 (88.008)\n","Epoch: [17][0/289]\tTime 0.193 (0.193)\tData 0.007 (0.007)\tLoss 0.3085 (0.3085)\tAccuracy 87.500 (87.500)\n","Epoch: [17][288/289]\tTime 0.147 (0.208)\tData 0.005 (0.006)\tLoss 0.2329 (0.2825)\tAccuracy 92.969 (88.111)\n","Epoch: [18][0/289]\tTime 0.178 (0.178)\tData 0.007 (0.007)\tLoss 0.2892 (0.2892)\tAccuracy 85.938 (85.938)\n","Epoch: [18][288/289]\tTime 0.297 (0.202)\tData 0.008 (0.006)\tLoss 0.2809 (0.2824)\tAccuracy 89.062 (88.181)\n","Epoch: [19][0/289]\tTime 0.171 (0.171)\tData 0.007 (0.007)\tLoss 0.2990 (0.2990)\tAccuracy 88.281 (88.281)\n","Epoch: [19][288/289]\tTime 0.616 (0.213)\tData 0.014 (0.006)\tLoss 0.3582 (0.2812)\tAccuracy 86.719 (88.165)\n","NLL Loss is 0.3413901695717372\n","Brier score is 0.0010235532344831317\n","Training time: 1202.9481461048126 seconds\n","Test: [0/293]\tTime 0.006 (0.006)\tLoss 0.1467 (0.1467)\tAccuracy 96.875 (96.875)\n","Test: [292/293]\tTime 0.004 (0.004)\tLoss 0.4652 (0.2959)\tAccuracy 78.125 (87.127)\n","test accuracy 87.12670648464164\n","Epoch: [0][0/289]\tTime 0.155 (0.155)\tData 0.006 (0.006)\tLoss 1.4491 (1.4491)\tAccuracy 84.375 (84.375)\n","Epoch: [0][288/289]\tTime 0.288 (0.214)\tData 0.007 (0.006)\tLoss 0.7884 (1.0053)\tAccuracy 82.031 (78.598)\n","Epoch: [1][0/289]\tTime 0.145 (0.145)\tData 0.007 (0.007)\tLoss 0.9809 (0.9809)\tAccuracy 71.875 (71.875)\n","Epoch: [1][288/289]\tTime 0.320 (0.213)\tData 0.008 (0.006)\tLoss 0.3596 (0.4942)\tAccuracy 85.156 (82.977)\n","Epoch: [2][0/289]\tTime 0.170 (0.170)\tData 0.008 (0.008)\tLoss 0.2777 (0.2777)\tAccuracy 91.406 (91.406)\n","Epoch: [2][288/289]\tTime 0.190 (0.218)\tData 0.006 (0.006)\tLoss 0.3346 (0.3361)\tAccuracy 83.594 (86.111)\n","Epoch: [3][0/289]\tTime 0.112 (0.112)\tData 0.005 (0.005)\tLoss 0.3181 (0.3181)\tAccuracy 84.375 (84.375)\n","Epoch: [3][288/289]\tTime 0.197 (0.224)\tData 0.006 (0.006)\tLoss 0.3274 (0.3175)\tAccuracy 85.938 (86.708)\n","Epoch: [4][0/289]\tTime 0.337 (0.337)\tData 0.009 (0.009)\tLoss 0.3448 (0.3448)\tAccuracy 86.719 (86.719)\n","Epoch: [4][288/289]\tTime 0.135 (0.224)\tData 0.005 (0.006)\tLoss 0.4810 (0.3095)\tAccuracy 76.562 (87.140)\n","Epoch: [5][0/289]\tTime 0.181 (0.181)\tData 0.006 (0.006)\tLoss 0.4100 (0.4100)\tAccuracy 78.906 (78.906)\n","Epoch: [5][288/289]\tTime 0.131 (0.214)\tData 0.005 (0.006)\tLoss 0.2018 (0.3079)\tAccuracy 92.188 (87.105)\n","Epoch: [6][0/289]\tTime 0.155 (0.155)\tData 0.007 (0.007)\tLoss 0.2783 (0.2783)\tAccuracy 85.938 (85.938)\n","Epoch: [6][288/289]\tTime 0.145 (0.209)\tData 0.005 (0.006)\tLoss 0.2946 (0.3020)\tAccuracy 87.500 (87.492)\n","Epoch: [7][0/289]\tTime 0.098 (0.098)\tData 0.005 (0.005)\tLoss 0.2940 (0.2940)\tAccuracy 85.156 (85.156)\n","Epoch: [7][288/289]\tTime 0.185 (0.209)\tData 0.006 (0.006)\tLoss 0.2967 (0.2995)\tAccuracy 86.719 (87.478)\n","Epoch: [8][0/289]\tTime 0.238 (0.238)\tData 0.009 (0.009)\tLoss 0.2894 (0.2894)\tAccuracy 88.281 (88.281)\n","Epoch: [8][288/289]\tTime 0.124 (0.207)\tData 0.004 (0.006)\tLoss 0.3348 (0.2977)\tAccuracy 88.281 (87.535)\n","Epoch: [9][0/289]\tTime 0.538 (0.538)\tData 0.013 (0.013)\tLoss 0.3156 (0.3156)\tAccuracy 86.719 (86.719)\n","Epoch: [9][288/289]\tTime 0.128 (0.207)\tData 0.005 (0.006)\tLoss 0.1948 (0.2983)\tAccuracy 91.406 (87.638)\n","Epoch: [10][0/289]\tTime 0.192 (0.192)\tData 0.007 (0.007)\tLoss 0.3255 (0.3255)\tAccuracy 85.156 (85.156)\n","Epoch: [10][288/289]\tTime 0.362 (0.206)\tData 0.008 (0.006)\tLoss 0.2610 (0.2945)\tAccuracy 88.281 (87.797)\n","Epoch: [11][0/289]\tTime 0.335 (0.335)\tData 0.010 (0.010)\tLoss 0.2298 (0.2298)\tAccuracy 91.406 (91.406)\n","Epoch: [11][288/289]\tTime 0.128 (0.208)\tData 0.004 (0.006)\tLoss 0.2342 (0.2910)\tAccuracy 91.406 (87.870)\n","Epoch: [12][0/289]\tTime 0.125 (0.125)\tData 0.006 (0.006)\tLoss 0.3209 (0.3209)\tAccuracy 86.719 (86.719)\n","Epoch: [12][288/289]\tTime 0.167 (0.206)\tData 0.006 (0.006)\tLoss 0.3510 (0.2915)\tAccuracy 85.156 (87.865)\n","Epoch: [13][0/289]\tTime 0.096 (0.096)\tData 0.005 (0.005)\tLoss 0.2964 (0.2964)\tAccuracy 87.500 (87.500)\n","Epoch: [13][288/289]\tTime 0.152 (0.203)\tData 0.006 (0.006)\tLoss 0.3140 (0.2882)\tAccuracy 85.938 (87.962)\n","Epoch: [14][0/289]\tTime 0.150 (0.150)\tData 0.007 (0.007)\tLoss 0.3599 (0.3599)\tAccuracy 88.281 (88.281)\n","Epoch: [14][288/289]\tTime 0.127 (0.207)\tData 0.004 (0.006)\tLoss 0.2233 (0.2861)\tAccuracy 88.281 (88.160)\n","Epoch: [15][0/289]\tTime 0.585 (0.585)\tData 0.014 (0.014)\tLoss 0.3413 (0.3413)\tAccuracy 85.938 (85.938)\n","Epoch: [15][288/289]\tTime 0.268 (0.210)\tData 0.007 (0.006)\tLoss 0.2277 (0.2859)\tAccuracy 89.844 (88.054)\n","Epoch: [16][0/289]\tTime 0.145 (0.145)\tData 0.007 (0.007)\tLoss 0.2530 (0.2530)\tAccuracy 89.844 (89.844)\n","Epoch: [16][288/289]\tTime 0.100 (0.207)\tData 0.004 (0.006)\tLoss 0.3103 (0.2839)\tAccuracy 89.062 (88.097)\n","Epoch: [17][0/289]\tTime 0.119 (0.119)\tData 0.005 (0.005)\tLoss 0.2358 (0.2358)\tAccuracy 90.625 (90.625)\n","Epoch: [17][288/289]\tTime 0.192 (0.208)\tData 0.006 (0.006)\tLoss 0.2963 (0.2840)\tAccuracy 89.844 (88.306)\n","Epoch: [18][0/289]\tTime 0.194 (0.194)\tData 0.007 (0.007)\tLoss 0.2710 (0.2710)\tAccuracy 88.281 (88.281)\n","Epoch: [18][288/289]\tTime 0.149 (0.211)\tData 0.005 (0.006)\tLoss 0.3054 (0.2820)\tAccuracy 82.031 (88.195)\n","Epoch: [19][0/289]\tTime 0.150 (0.150)\tData 0.007 (0.007)\tLoss 0.3374 (0.3374)\tAccuracy 85.156 (85.156)\n","Epoch: [19][288/289]\tTime 0.134 (0.214)\tData 0.004 (0.006)\tLoss 0.2704 (0.2806)\tAccuracy 91.406 (88.206)\n","NLL Loss is 0.3417923002171888\n","Brier score is 0.00101837897383218\n","Training time: 1219.6907804012299 seconds\n","Test: [0/293]\tTime 0.006 (0.006)\tLoss 0.1538 (0.1538)\tAccuracy 96.875 (96.875)\n","Test: [292/293]\tTime 0.004 (0.005)\tLoss 0.4150 (0.2950)\tAccuracy 78.125 (87.255)\n","test accuracy 87.2546928327645\n","Epoch: [0][0/289]\tTime 0.172 (0.172)\tData 0.006 (0.006)\tLoss 1.7030 (1.7030)\tAccuracy 28.125 (28.125)\n","Epoch: [0][288/289]\tTime 0.329 (0.211)\tData 0.008 (0.006)\tLoss 0.9365 (0.9940)\tAccuracy 75.781 (78.482)\n","Epoch: [1][0/289]\tTime 0.114 (0.114)\tData 0.006 (0.006)\tLoss 0.9469 (0.9469)\tAccuracy 77.344 (77.344)\n","Epoch: [1][288/289]\tTime 0.164 (0.214)\tData 0.005 (0.006)\tLoss 0.3269 (0.5197)\tAccuracy 88.281 (82.618)\n","Epoch: [2][0/289]\tTime 0.175 (0.175)\tData 0.007 (0.007)\tLoss 0.3475 (0.3475)\tAccuracy 83.594 (83.594)\n","Epoch: [2][288/289]\tTime 0.138 (0.220)\tData 0.005 (0.006)\tLoss 0.3749 (0.3372)\tAccuracy 86.719 (85.965)\n","Epoch: [3][0/289]\tTime 0.160 (0.160)\tData 0.006 (0.006)\tLoss 0.2462 (0.2462)\tAccuracy 89.844 (89.844)\n","Epoch: [3][288/289]\tTime 0.177 (0.225)\tData 0.006 (0.006)\tLoss 0.3523 (0.3181)\tAccuracy 85.938 (86.603)\n","Epoch: [4][0/289]\tTime 0.221 (0.221)\tData 0.008 (0.008)\tLoss 0.3253 (0.3253)\tAccuracy 86.719 (86.719)\n","Epoch: [4][288/289]\tTime 0.289 (0.225)\tData 0.007 (0.006)\tLoss 0.3102 (0.3088)\tAccuracy 86.719 (87.070)\n","Epoch: [5][0/289]\tTime 0.180 (0.180)\tData 0.007 (0.007)\tLoss 0.3312 (0.3312)\tAccuracy 85.156 (85.156)\n","Epoch: [5][288/289]\tTime 0.211 (0.217)\tData 0.006 (0.006)\tLoss 0.3154 (0.3039)\tAccuracy 82.812 (87.251)\n","Epoch: [6][0/289]\tTime 0.198 (0.198)\tData 0.007 (0.007)\tLoss 0.2763 (0.2763)\tAccuracy 85.938 (85.938)\n","Epoch: [6][288/289]\tTime 0.156 (0.219)\tData 0.004 (0.006)\tLoss 0.3517 (0.3011)\tAccuracy 82.812 (87.411)\n","Epoch: [7][0/289]\tTime 0.175 (0.175)\tData 0.007 (0.007)\tLoss 0.2284 (0.2284)\tAccuracy 89.062 (89.062)\n","Epoch: [7][288/289]\tTime 0.129 (0.215)\tData 0.005 (0.006)\tLoss 0.2854 (0.2963)\tAccuracy 84.375 (87.519)\n","Epoch: [8][0/289]\tTime 0.125 (0.125)\tData 0.006 (0.006)\tLoss 0.3794 (0.3794)\tAccuracy 81.250 (81.250)\n","Epoch: [8][288/289]\tTime 0.110 (0.209)\tData 0.004 (0.006)\tLoss 0.2934 (0.2929)\tAccuracy 86.719 (87.662)\n","Epoch: [9][0/289]\tTime 0.226 (0.226)\tData 0.007 (0.007)\tLoss 0.3248 (0.3248)\tAccuracy 83.594 (83.594)\n","Epoch: [9][288/289]\tTime 0.293 (0.215)\tData 0.007 (0.006)\tLoss 0.2608 (0.2917)\tAccuracy 88.281 (87.695)\n","Epoch: [10][0/289]\tTime 0.121 (0.121)\tData 0.006 (0.006)\tLoss 0.3192 (0.3192)\tAccuracy 86.719 (86.719)\n","Epoch: [10][288/289]\tTime 0.144 (0.210)\tData 0.005 (0.006)\tLoss 0.2258 (0.2895)\tAccuracy 92.969 (87.935)\n","Epoch: [11][0/289]\tTime 0.144 (0.144)\tData 0.006 (0.006)\tLoss 0.2864 (0.2864)\tAccuracy 87.500 (87.500)\n","Epoch: [11][288/289]\tTime 0.320 (0.206)\tData 0.008 (0.006)\tLoss 0.2517 (0.2875)\tAccuracy 88.281 (87.957)\n","Epoch: [12][0/289]\tTime 0.123 (0.123)\tData 0.006 (0.006)\tLoss 0.3155 (0.3155)\tAccuracy 83.594 (83.594)\n","Epoch: [12][288/289]\tTime 0.124 (0.209)\tData 0.005 (0.006)\tLoss 0.3280 (0.2853)\tAccuracy 84.375 (88.046)\n","Epoch: [13][0/289]\tTime 0.215 (0.215)\tData 0.008 (0.008)\tLoss 0.3448 (0.3448)\tAccuracy 85.938 (85.938)\n","Epoch: [13][288/289]\tTime 0.123 (0.208)\tData 0.004 (0.006)\tLoss 0.3291 (0.2837)\tAccuracy 86.719 (88.119)\n","Epoch: [14][0/289]\tTime 0.150 (0.150)\tData 0.007 (0.007)\tLoss 0.3010 (0.3010)\tAccuracy 89.844 (89.844)\n","Epoch: [14][288/289]\tTime 0.192 (0.208)\tData 0.006 (0.006)\tLoss 0.2032 (0.2822)\tAccuracy 92.188 (88.106)\n","Epoch: [15][0/289]\tTime 0.098 (0.098)\tData 0.005 (0.005)\tLoss 0.3560 (0.3560)\tAccuracy 82.031 (82.031)\n","Epoch: [15][288/289]\tTime 0.142 (0.202)\tData 0.004 (0.006)\tLoss 0.2532 (0.2796)\tAccuracy 90.625 (88.252)\n","Epoch: [16][0/289]\tTime 0.119 (0.119)\tData 0.006 (0.006)\tLoss 0.1464 (0.1464)\tAccuracy 96.094 (96.094)\n","Epoch: [16][288/289]\tTime 0.238 (0.208)\tData 0.007 (0.006)\tLoss 0.2882 (0.2811)\tAccuracy 85.938 (88.284)\n","Epoch: [17][0/289]\tTime 0.194 (0.194)\tData 0.008 (0.008)\tLoss 0.2734 (0.2734)\tAccuracy 89.062 (89.062)\n","Epoch: [17][288/289]\tTime 0.191 (0.208)\tData 0.006 (0.006)\tLoss 0.2823 (0.2785)\tAccuracy 88.281 (88.297)\n","Epoch: [18][0/289]\tTime 0.232 (0.232)\tData 0.009 (0.009)\tLoss 0.2554 (0.2554)\tAccuracy 89.062 (89.062)\n","Epoch: [18][288/289]\tTime 0.199 (0.206)\tData 0.006 (0.006)\tLoss 0.3439 (0.2776)\tAccuracy 84.375 (88.373)\n","Epoch: [19][0/289]\tTime 0.145 (0.145)\tData 0.007 (0.007)\tLoss 0.2504 (0.2504)\tAccuracy 89.844 (89.844)\n","Epoch: [19][288/289]\tTime 0.315 (0.206)\tData 0.008 (0.006)\tLoss 0.2757 (0.2769)\tAccuracy 88.281 (88.279)\n","NLL Loss is 0.33928214118245564\n","Brier score is 0.0010159291198096887\n","Training time: 1225.3910310268402 seconds\n","Test: [0/293]\tTime 0.005 (0.005)\tLoss 0.1370 (0.1370)\tAccuracy 96.875 (96.875)\n","Test: [292/293]\tTime 0.003 (0.004)\tLoss 0.4046 (0.2915)\tAccuracy 78.125 (87.532)\n","test accuracy 87.53199658703072\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e29KNor6Mue9"},"source":["\n","def evaluate(model, device, data_loader, criterion, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\n","\tresults = []\n","\n","\tmodel.eval()\n","\n","\twith torch.no_grad():\n","\t\tend = time.time()\n","\t\tfor i, (input, target) in enumerate(data_loader):\n","\t\t\tseqs, lengths = input\n","\t\t\tseqs = seqs.to(device)\n","      \n","\t\t\ttarget = target.to(device)\n","\n","\t\t\toutput = model(seqs, lengths)\n","\t\t\tloss = criterion(output, target)\n","\n","\t\t\t# measure elapsed time\n","\t\t\tbatch_time.update(time.time() - end)\n","\t\t\tend = time.time()\n","\n","\t\t\tlosses.update(loss.item(), target.size(0))\n","\t\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\t\ty_true = target.detach().to('cpu').numpy().tolist()\n","\t\t\ty_pred = output.detach().to('cpu').max(1)[1].numpy().tolist()\n","\t\t\tresults.extend(list(zip(y_true, y_pred)))\n","\n","\t\t\tif i % print_freq == 0:\n","\t\t\t\tprint('Test: [{0}/{1}]\\t'\n","\t\t\t\t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t\t\t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t\t\t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t\t\t\ti, len(data_loader), batch_time=batch_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, results\n","\n","def advEvaluate(model, device, data_loader, criterion, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\teps = 5067*0.01\n","\tresults = []\n","\n","\tmodel.eval()\n","\n","\twith torch.no_grad():\n","\t\tend = time.time()\n","\t\tfor i, (input, target) in enumerate(data_loader):\n","\t\t\tseqs, lengths = input\n","\n","\t\t\tseqs = seqs.to(device) \n","\t\t\tseqs = seqs + eps*(torch.sign(seqs))\n","\n","\t\t\ttarget = target.to(device)\n","\n","\t\t\toutput = model(seqs, lengths)\n","\t\t\tloss = criterion(output, target)\n","\n","\t\t\t# measure elapsed time\n","\t\t\tbatch_time.update(time.time() - end)\n","\t\t\tend = time.time()\n","\n","\t\t\tlosses.update(loss.item(), target.size(0))\n","\t\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\t\ty_true = target.detach().to('cpu').numpy().tolist()\n","\t\t\ty_pred = output.detach().to('cpu').max(1)[1].numpy().tolist()\n","\t\t\tresults.extend(list(zip(y_true, y_pred)))\n","\n","\t\t\tif i % print_freq == 0:\n","\t\t\t\tprint('Test: [{0}/{1}]\\t'\n","\t\t\t\t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t\t\t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t\t\t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t\t\t\ti, len(data_loader), batch_time=batch_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cixUfpdsdaIO"},"source":["Reference: https://github.com/giannifranchi/LP_BNN/blob/d324ba8d0ade75e5bfe9a14c670fe71469f49db6/networks/batchensemble_layers.py"]}]}