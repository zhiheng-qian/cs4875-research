{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepEnsemble.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfgudWUpgoJy1H9zaGYtAt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sH1ckzOtWxNw"},"source":["<h1>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles<h1/>\n","\n","Balaji Lakshminarayanan Alexander Pritzel Charles Blundell"]},{"cell_type":"markdown","metadata":{"id":"WazVtZ1MagbT"},"source":["<h2>Classification with Wide ResNet and CIFAR10<h2/>"]},{"cell_type":"code","metadata":{"id":"NHtidof0Zxv_"},"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","m = nn.Softplus()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zb-YVXJ_au9e","executionInfo":{"status":"ok","timestamp":1616587380939,"user_tz":-120,"elapsed":23763,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"1d852a48-f04c-480c-bdca-e392a24b6ec3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CVf-V55ugfUu"},"source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate, stride=1):\n","        \"\"\"\n","        Args:\n","          in_channels:  Number of input channels.\n","          out_channels: Number of output channels.\n","          dropout_rate:  Dropout Rate\n","          stride:       Controls the stride.\n","        \"\"\"\n","        super(Block, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.BatchNorm2d(in_channels),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, bias=False, padding = 1),\n","            nn.Dropout(p = dropout_rate),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace = True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, bias=False, stride = stride, padding = 1)\n","        )\n","        self.skip = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.skip = nn.Sequential(\n","               nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","            )\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out += self.skip(x)\n","        return out\n","\n","class GroupOfBlocks(nn.Module):\n","    def __init__(self, in_channels, out_channels, n_blocks, dropout_rate, stride=1):\n","        super(GroupOfBlocks, self).__init__()\n","        strides = [stride] + [1]*(int(n_blocks) - 1)\n","        self.in_channels = in_channels\n","        group = []\n","\n","        for stride in strides:\n","            group.append(Block(self.in_channels, out_channels, dropout_rate, stride))\n","            self.in_channels = out_channels\n","\n","        self.group = nn.Sequential(*group)\n","\n","    def forward(self, x):\n","        return self.group(x)\n","\n","class WideResNet(nn.Module):\n","    def __init__(self, depth, widen_factor, dropout_rate, num_classes=10):\n","        super(WideResNet, self).__init__()\n","        assert ((depth-4)%6 == 0), \"Depth should be 6n+4.\"\n","        n = (depth - 4)/6\n","        k = widen_factor\n","        nStages = [16, 16*k, 32*k, 64*k]\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n","        self.group1 = GroupOfBlocks(nStages[0], nStages[1], n, dropout_rate)\n","        self.group2 = GroupOfBlocks(nStages[1], nStages[2], n, dropout_rate, stride=2)\n","        self.group3 = GroupOfBlocks(nStages[2], nStages[3], n, dropout_rate, stride=2)\n","        self.bn1 = nn.BatchNorm2d(nStages[3])\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc = nn.Linear(nStages[3], num_classes)\n","        self.nStage3 = nStages[3]\n","\n","        # Initialize weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, np.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","\n","        x = self.group1(x)\n","        x = self.group2(x)\n","        x = self.group3(x)\n","        x = self.relu(self.bn1(x))\n","        x = F.avg_pool2d(x, 8)\n","        x = x.view(-1, self.nStage3)\n","        return self.fc(x)\n","\n","# This function computes the accuracy on the test dataset\n","def compute_accuracy_ood(net, testloader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images, labels = images.to(device), labels.to(device)\n","            images = images.clone().detach().requires_grad_(True)\n","            images = images + eps*(torch.sign(images))\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return correct / total\n","\n","def compute_accuracy(net, testloader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return correct / total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2uYIkcKtfgp","executionInfo":{"status":"ok","timestamp":1614616030809,"user_tz":-120,"elapsed":10568,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"1df57686-20e2-402f-8863-aa44b21701c4"},"source":["import torchvision\n","import torchvision.transforms as transforms\n","data_dir = '/content/drive/My Drive/AALTO/cs4875-research/data/'\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Transform to tensor\n","    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZkD56pZE5GjY"},"source":["<h2>Ensemble without adversarial training<h2/>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"id":"7dvJCl9k5FTs","executionInfo":{"status":"error","timestamp":1610294729183,"user_tz":-120,"elapsed":265825,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"ac760596-6fdb-4905-b1e9-6e6d6bfb321a"},"source":["device = torch.device('cuda:0')\n","loss_func = nn.CrossEntropyLoss()\n","m = nn.LogSoftmax(dim=1)\n","eps = 0.01*2 # input ranges from (-1, 1)\n","learning_rate = 0.01\n","\n","def compute_brier_score(p, y):\n","  brier_score = torch.mean((y-torch.argmax(p, 1).float())**2)\n","  return brier_score\n","\n","def ensembleWithoutAdversarial(model, optimizer):\n","  running_loss = 0.0\n","  running_brier = 0.0\n","  model.train()\n","  for epoch in range(numEpochs):\n","    brier_score = 0.0\n","    total = 0\n","    for x, y in trainloader:\n","        x, y = x.to(device), y.to(device)\n","        x = x.clone().detach().requires_grad_(True)\n","        optimizer.zero_grad()\n","        output = model(x)\n","        batch_brier_score = compute_brier_score(output, y)\n","        brier_score += torch.sum(batch_brier_score, 0).cpu().numpy().item()\n","        loss = loss_func(output, y)\n","        loss.backward()\n","        optimizer.step()\n","        total += y.size(0)\n","    if epoch == (numEpochs-1):\n","      running_loss = loss.item()\n","    if epoch == (numEpochs-1):\n","      print('Loss at epoch {} is {}'.format(epoch, loss.item()))\n","      print('Brier score at epoch {} is {}'.format(epoch, brier_score/total))\n","  return running_loss, brier_score/total\n","\n","\n","numEpochs = 40\n","training_loss = []\n","training_brier = []\n","accuracys = []\n","accuracy_oods = []\n","time_all = []\n","for i in range(4):\n","  model = WideResNet(28, 4, 0.5)\n","  model.to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  t0 = time.time()\n","  loss, brier = ensembleWithoutAdversarial(model, optimizer)\n","  time_one = time.time() - t0\n","  training_loss.append(loss)\n","  training_brier.append(brier)\n","  accuracy = compute_accuracy(model, testloader)\n","  accuracy_ood = compute_accuracy_ood(model, testloader)\n","\n","  accuracys.append(accuracy)\n","  accuracy_oods.append(accuracy_ood)\n","  time_all.append(time_one)\n","  print('Accuracy of the network on the test images: %.3f' % accuracy)\n","  print('Accuracy of the network on the OOD test images: %.3f' % accuracy_ood)\n","  print('NLL Loss is {}'.format(loss))\n","  print('Brier score is {}'.format(brier))\n","  print('Training time: {} seconds'.format(time_one))\n","print('Mean:')\n","print('Accuracy of the network on the test images: %.3f' % np.mean(accuracys))\n","print('Accuracy of the network on the OOD test images: %.3f' % np.mean(accuracy_oods))\n","print('NLL Loss is {}'.format(np.mean(training_loss)))\n","print('Brier score is {}'.format(np.mean(training_brier)))\n","print('Training time: {} seconds'.format(np.sum(time_all)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loss at epoch 0 is 1.8651790618896484\n","Brier score at epoch 0 is 0.399615\n","Accuracy of the network on the test images: 0.167\n","Accuracy of the network on the OOD test images: 0.168\n","NLL Loss is [1.8651790618896484]\n","Brier score is [0.399615]\n","Training time: 250.71372509002686 seconds\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-a7debc407b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensembleWithAdversarial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m   \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mtraining_brier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-a7debc407b99>\u001b[0m in \u001b[0;36mensembleWithAdversarial\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"KAXr_qQN6L5C"},"source":["<h2>Ensemble with adversarial training<h2/>"]},{"cell_type":"code","metadata":{"id":"638YJQZKTvaf"},"source":["device = torch.device('cuda:0')\n","loss_func = nn.CrossEntropyLoss()\n","m = nn.LogSoftmax(dim=1)\n","eps = 0.01*2 # input ranges from (-1, 1)\n","learning_rate = 0.01\n","\n","def compute_brier_score(p, y):\n","  brier_score = torch.mean((y-torch.argmax(p, 1).float())**2)\n","  return brier_score\n","\n","def ensembleWithAdversarial(model, optimizer):\n","  running_loss = 0.0\n","  running_brier = 0.0\n","  model.train()\n","  for epoch in range(numEpochs):\n","    brier_score = 0.0\n","    total = 0\n","    for x, y in trainloader:\n","        x, y = x.to(device), y.to(device)\n","        x = x.clone().detach().requires_grad_(True)\n","        optimizer.zero_grad()\n","        output = model(x)\n","        batch_brier_score = compute_brier_score(output, y)\n","        brier_score += torch.sum(batch_brier_score, 0).cpu().numpy().item()\n","        loss = loss_func(output, y)\n","        loss.backward(retain_graph=True)\n","        x_prime = x + eps*(torch.sign(x.grad.data))\n","        optimizer.zero_grad()\n","        output_prime = model(x_prime)\n","        loss = loss_func(output, y) + loss_func(output_prime, y)\n","        loss.backward()\n","        optimizer.step()\n","        total += y.size(0)\n","    if epoch == (numEpochs-1):\n","      running_loss = loss.item()\n","    if epoch == (numEpochs-1):\n","      print('Loss at epoch {} is {}'.format(epoch, loss.item()))\n","      print('Brier score at epoch {} is {}'.format(epoch, brier_score/total))\n","  return running_loss, brier_score/total\n","\n","\n","numEpochs = 40\n","training_loss = []\n","training_brier = []\n","accuracys = []\n","accuracy_oods = []\n","time_all = []\n","for i in range(4):\n","  model = WideResNet(28, 4, 0.5)\n","  model.to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  t0 = time.time()\n","  loss, brier = ensembleWithAdversarial(model, optimizer)\n","  time_one = time.time() - t0\n","  training_loss.append(loss)\n","  training_brier.append(brier)\n","  accuracy = compute_accuracy(model, testloader)\n","  accuracy_ood = compute_accuracy_ood(model, testloader)\n","  accuracys.append(accuracy)\n","  accuracy_oods.append(accuracy_ood)\n","  time_all.append(time_one)\n","  print('Accuracy of the network on the test images: %.3f' % accuracy)\n","  print('Accuracy of the network on the OOD test images: %.3f' % accuracy_ood)\n","  print('NLL Loss is {}'.format(loss))\n","  print('Brier score is {}'.format(brier))\n","  print('Training time: {} seconds'.format(time_one))\n","print('Mean:')\n","print('Accuracy of the network on the test images: %.3f' % np.mean(accuracys))\n","print('Accuracy of the network on the OOD test images: %.3f' % np.mean(accuracy_oods))\n","print('NLL Loss is {}'.format(np.mean(training_loss)))\n","print('Brier score is {}'.format(np.mean(training_brier)))\n","print('Training time: {} seconds'.format(np.sum(time_all)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALWpDQTlb-22"},"source":["<h2>Time series prediction with MIMIC3 and LSTM<h2/>"]},{"cell_type":"code","metadata":{"id":"dLbQRtqCua6l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616587432037,"user_tz":-120,"elapsed":9962,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"c15ec1ee-6f62-4d3e-be7a-8aa44170244e"},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from mydatasets import calculate_num_features, VisitSequenceWithLabelDataset, visit_collate_fn\n","# !pip3 install pickle5\n","import pickle5 as pickle\n","from torch.utils.data import DataLoader\n","import torch.optim as optim"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pickle5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n","\r\u001b[K     |██▌                             | 10kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.1MB/s \n","\u001b[?25hBuilding wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219261 sha256=2a8a861919892ce3bd92c6b7fe2a5cc03894cc540dbaced385c1b361c9fc6f3d\n","  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BCGXxPtHl6iM"},"source":["# Data preprocessing and training process refers to https://github.com/jiaweizhu830/Time-Series-Mortality-Prediction-in-ICU-via-PyTorch\n","# Train : test = 8:2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udg0WWmzjCTK"},"source":["class AverageMeter(object):\n","\t\"\"\"Computes and stores the average and current value\"\"\"\n","\n","\tdef __init__(self):\n","\t\tself.reset()\n","\n","\tdef reset(self):\n","\t\tself.val = 0\n","\t\tself.avg = 0\n","\t\tself.sum = 0\n","\t\tself.count = 0\n","\n","\tdef update(self, val, n=1):\n","\t\tself.val = val\n","\t\tself.sum += val * n\n","\t\tself.count += n\n","\t\tself.avg = self.sum / self.count\n","\n","def compute_brier_score(p, y):\n","  brier_score = torch.mean((y-torch.argmax(p, 1).float())**2)\n","  return brier_score\n","\n","def compute_batch_accuracy(output, target):\n","\t\"\"\"Computes the accuracy for a batch\"\"\"\n","\twith torch.no_grad():\n","\n","\t\tbatch_size = target.size(0)\n","\t\t_, pred = output.max(1)\n","\t\tcorrect = pred.eq(target).sum()\n","\n","\t\treturn correct * 100.0 / batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQqyYKW-oOrU","executionInfo":{"status":"ok","timestamp":1616587480702,"user_tz":-120,"elapsed":45849,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"bd1bf963-2cc2-4ee6-f054-0a190a08e80b"},"source":["\n","torch.manual_seed(0)\n","if torch.cuda.is_available():\n","\ttorch.cuda.manual_seed(0)\n","\n","# Set a correct path to the data files that you preprocessed\n","PATH_TRAIN_SEQS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/train/mortality.seqs.train\"\n","PATH_TRAIN_LABELS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/train/mortality.labels.train\"\n","PATH_TEST_SEQS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/test/mortality.seqs.test\"\n","PATH_TEST_LABELS = \"/content/drive/My Drive/AALTO/cs4875-research/data/features/test/mortality.labels.test\"\n","PATH_OUTPUT = \"/content/drive/My Drive/AALTO/cs4875-research/output/\"\n","\n","NUM_EPOCHS = 1\n","BATCH_SIZE = 32\n","USE_CUDA = False  # Set 'True' if you want to use GPU\n","NUM_WORKERS = 0\n","\n","# Data loading\n","print('===> Loading entire datasets')\n","train_seqs = pickle.load(open(PATH_TRAIN_SEQS, 'rb'))\n","train_labels = pickle.load(open(PATH_TRAIN_LABELS, 'rb'))\n","test_seqs = pickle.load(open(PATH_TEST_SEQS, 'rb'))\n","test_labels = pickle.load(open(PATH_TEST_LABELS, 'rb'))\n","print('===> done Loading')\n","num_features = calculate_num_features(train_seqs)\n","print(num_features)\n","\n","train_dataset = VisitSequenceWithLabelDataset(train_seqs, train_labels, num_features)\n","test_dataset = VisitSequenceWithLabelDataset(test_seqs, test_labels, num_features)\n","print('===> done datasets')\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=visit_collate_fn, num_workers=NUM_WORKERS)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, collate_fn=visit_collate_fn, num_workers=NUM_WORKERS)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["===> Loading entire datasets\n","===> done Loading\n","5067\n","===> done datasets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tORU8oPXY38_"},"source":["class MyLSTM(nn.Module):\n","    def __init__(self, dim_input, dropout_rate=0.1):\n","        super(MyLSTM, self).__init__()\n","        self.fc1 = nn.Linear(in_features = dim_input, out_features = 64)\n","        self.lstm = nn.LSTM(input_size = 64, hidden_size = 64, num_layers = 1, dropout = dropout_rate, batch_first = True)\n","        #self.dropout = nn.Dropout(p = dropout_rate)\n","        self.fc2 = nn.Linear(in_features = 64, out_features = 2)\n","\n","    def forward(self, x, lengths):\n","      # x, lengths = input_sequence\n","      lengths = lengths.long()\n","      batch_size, seq_len, num_features = x.size()\n","      # print('1 ' + str(x.size()))\n","      x = self.fc1(x)\n","      # print('1.5 ' + str(x.size()))\n","      x = torch.sigmoid(x)\n","      # print('2 ' + str(x.size()))\n","      x = pack_padded_sequence(x, lengths, batch_first = True)\n","      # print('3 ' + str(x.batch_sizes))\n","      x, _ = self.lstm(x)\n","      # print('4 ' + str(x.batch_sizes))\n","      x, _ = pad_packed_sequence(x, batch_first = True, total_length = seq_len)\n","      # print('5 ' + str(x.size()))\n","      y = torch.zeros(batch_size, 64).float()\n","      for i in range(batch_size):\n","        y[i, :] = x[i, lengths[i]-1, :]\n","      # print('5 y ' + str(y.size()))\n","      x = self.fc2(y)\n","      # print('6 ' + str(x.size()))\n","      # assert (False), \"Stop here.\"\n","      return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDxHYuzTuJRX"},"source":["\n","def train(model, device, data_loader, criterion, optimizer, epoch, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tdata_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\ttotal = 0\n","\tbrier_score = 0.0\n","\tmodel.train()\n","\n","\tend = time.time()\n","\tfor i, (input, target) in enumerate(data_loader):\n","\t\t# measure data loading time\n","\t\tdata_time.update(time.time() - end)\n","\n","\t\tseqs, lengths = input\n","\t\tseqs = seqs.to(device)\n","\t\ttarget = target.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\toutput = model(seqs, lengths)\n","\t\tbatch_brier_score = compute_brier_score(output, target)\n","\t\tbrier_score+= torch.sum(batch_brier_score, 0).cpu().numpy().item()\n","\t\ttotal += target.size(0)\n","\t\tloss = criterion(output, target)\n","\t\tassert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n","\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\n","\t\t# measure elapsed time\n","\t\tbatch_time.update(time.time() - end)\n","\t\tend = time.time()\n","\n","\t\tlosses.update(loss.item(), target.size(0))\n","\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\t# if i % print_freq == 0:\n","\t\t# \tprint('Epoch: [{0}][{1}/{2}]\\t'\n","\t\t# \t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t# \t\t  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","\t\t# \t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t# \t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t# \t\tepoch, i, len(data_loader), batch_time=batch_time,\n","\t\t# \t\tdata_time=data_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, brier_score/total\n","\n","def advTrain(model, device, data_loader, criterion, optimizer, epoch, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tdata_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\teps = 5067*0.01\n","\ttotal = 0\n","\tbrier_score = 0.0\n","\n","\tmodel.train()\n","\n","\tend = time.time()\n","\n","\tfor i, (input, target) in enumerate(data_loader):\n","\t\t# measure data loading time\n","\t\tdata_time.update(time.time() - end)\n","\n","\t\tseqs, lengths = input\n","\t\tseqs = seqs.to(device)\n","\t\tseqs = seqs.clone().detach().requires_grad_(True)\n","    \n","\t\ttarget = target.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\toutput = model(seqs, lengths)\n","\t\tbatch_brier_score = compute_brier_score(output, target)\n","\t\tbrier_score+= torch.sum(batch_brier_score, 0).cpu().numpy().item()\n","\t\ttotal += target.size(0)\n","\t\tloss = criterion(output, target)\n","\t\tassert not np.isnan(loss.item()), 'Model diverged with loss = NaN'\n","\n","\t\tloss.backward(retain_graph=True)\n","\n","\t\tseqs_prime = seqs + eps*(torch.sign(seqs.grad.data))\n","    \n","\t\toptimizer.zero_grad()\n","\t\toutput_prime = model(seqs_prime, lengths)\n","\t\tloss = criterion(output, target) + criterion(output_prime, target)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\n","\t\t# measure elapsed time\n","\t\tbatch_time.update(time.time() - end)\n","\t\tend = time.time()\n","\n","\t\tlosses.update(loss.item(), target.size(0))\n","\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\t# if i % print_freq == 0:\n","\t\t# \tprint('Epoch: [{0}][{1}/{2}]\\t'\n","\t\t# \t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t# \t\t  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","\t\t# \t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t# \t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t# \t\tepoch, i, len(data_loader), batch_time=batch_time,\n","\t\t# \t\tdata_time=data_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, brier_score/total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMaaG8_lrNEL","executionInfo":{"status":"ok","timestamp":1615456458219,"user_tz":-120,"elapsed":12838778,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"0193cd52-c375-4025-d9f3-28eba9c261bd"},"source":["for i in range(1, 4):\n","  for i in range(1, 5):\n","    model = MyLSTM(num_features)\n","    criterion = nn.CrossEntropyLoss()\n","    NUM_EPOCHS = 20\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_CUDA else \"cpu\")\n","    model.to(device)\n","    criterion.to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0004)\n","    best_val_acc = 0.0\n","    train_losses, train_accuracies = [], []\n","    valid_losses, valid_accuracies = [], []\n","    training_brier = []\n","    t0 = time.time()\n","    for epoch in range(NUM_EPOCHS):\n","\n","      train_loss, train_accuracy, brier = advTrain(model, device, train_loader, criterion, optimizer, epoch, print_freq = len(train_loader)-1)\n","\n","\n","      train_losses.append(train_loss)\n","      training_brier.append(brier)\n","      train_accuracies.append(train_accuracy)\n","    time_one = time.time() - t0\n","    print('NLL Loss is {}'.format(np.mean(train_losses)))\n","    print('Brier score is {}'.format(np.mean(training_brier)))\n","    print('Training time: {} seconds'.format(time_one))\n","    test_loss, test_accuracy, test_results = advEvaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","    print('OOD accuracy {}'.format(test_accuracy))\n","    test_loss, test_accuracy, test_results = evaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","    print('test accuracy {}'.format(test_accuracy))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["NLL Loss is 0.31624441407166676\n","Brier score is 0.0033095868338740573\n","Training time: 1049.9844000339508 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.2669 (0.7036)\tAccuracy 100.000 (77.393)\n","OOD accuracy 77.39259889408763\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0004 (0.0004)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.1301 (0.3286)\tAccuracy 100.000 (86.963)\n","test accuracy 86.9629944704381\n","NLL Loss is 0.3505539423413424\n","Brier score is 0.0033423735019666736\n","Training time: 1029.8340990543365 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0001 (0.0001)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0150 (0.6621)\tAccuracy 100.000 (79.785)\n","OOD accuracy 79.78519778817524\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0008 (0.0008)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0732 (0.3244)\tAccuracy 100.000 (86.559)\n","test accuracy 86.55891110165886\n","NLL Loss is 0.3330098046507111\n","Brier score is 0.003332522447940263\n","Training time: 1047.0715565681458 seconds\n","Test: [0/9404]\tTime 0.003 (0.003)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.3159 (0.6770)\tAccuracy 100.000 (75.659)\n","OOD accuracy 75.65929391748192\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0008 (0.0008)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.2148 (0.3166)\tAccuracy 100.000 (86.889)\n","test accuracy 86.88855806039983\n","NLL Loss is 0.31653098580782457\n","Brier score is 0.0032963156095465497\n","Training time: 1056.0749056339264 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.2004 (0.6683)\tAccuracy 100.000 (76.404)\n","OOD accuracy 76.40365801786474\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0005 (0.0005)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0513 (0.3308)\tAccuracy 100.000 (86.793)\n","test accuracy 86.79285410463632\n","NLL Loss is 0.30151587512025557\n","Brier score is 0.0033124710972482927\n","Training time: 1052.1344439983368 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0003 (0.6824)\tAccuracy 100.000 (78.275)\n","OOD accuracy 78.2752020416844\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0004 (0.0004)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0323 (0.3351)\tAccuracy 100.000 (86.251)\n","test accuracy 86.25053168864314\n","NLL Loss is 0.3073345219858628\n","Brier score is 0.0032770048943002775\n","Training time: 1091.1150245666504 seconds\n","Test: [0/9404]\tTime 0.006 (0.006)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0024 (0.6924)\tAccuracy 100.000 (79.030)\n","OOD accuracy 79.03019991492982\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0013 (0.0013)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0534 (0.3269)\tAccuracy 100.000 (86.772)\n","test accuracy 86.7715865589111\n","NLL Loss is 0.3266561492204682\n","Brier score is 0.0033181914525663783\n","Training time: 1055.2648193836212 seconds\n","Test: [0/9404]\tTime 0.003 (0.003)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0005 (0.6132)\tAccuracy 100.000 (81.136)\n","OOD accuracy 81.13568694172693\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0012 (0.0012)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0136 (0.3706)\tAccuracy 100.000 (86.527)\n","test accuracy 86.52700978307104\n","NLL Loss is 0.32081015106451366\n","Brier score is 0.0033265913427298135\n","Training time: 1051.9556322097778 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0041 (0.7423)\tAccuracy 100.000 (77.573)\n","OOD accuracy 77.57337303275202\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0007 (0.0007)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0338 (0.3245)\tAccuracy 100.000 (86.463)\n","test accuracy 86.46320714589537\n","NLL Loss is 0.3003142067211793\n","Brier score is 0.0033448422868995616\n","Training time: 1049.0700657367706 seconds\n","Test: [0/9404]\tTime 0.003 (0.003)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 1.7392 (0.9480)\tAccuracy 0.000 (71.693)\n","OOD accuracy 71.69289663972778\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0008 (0.0008)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.2552 (0.3336)\tAccuracy 100.000 (85.793)\n","test accuracy 85.79327945555083\n","NLL Loss is 0.3112057373945463\n","Brier score is 0.00329149244682291\n","Training time: 1076.897759437561 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.8557 (0.7524)\tAccuracy 0.000 (76.085)\n","OOD accuracy 76.0846448319864\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0005 (0.0005)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0872 (0.3384)\tAccuracy 100.000 (86.134)\n","test accuracy 86.1335601871544\n","NLL Loss is 0.37941785698346064\n","Brier score is 0.003315975567542513\n","Training time: 1059.4260988235474 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0002 (0.0002)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0009 (0.6080)\tAccuracy 100.000 (82.741)\n","OOD accuracy 82.74138664398129\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0014 (0.0014)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0338 (0.3373)\tAccuracy 100.000 (86.974)\n","test accuracy 86.97362824330072\n","NLL Loss is 0.33068901138181694\n","Brier score is 0.0033426203804519223\n","Training time: 1052.0555074214935 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0405 (0.6485)\tAccuracy 100.000 (80.264)\n","OOD accuracy 80.26371756699277\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0008 (0.0008)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.1420 (0.3294)\tAccuracy 100.000 (86.229)\n","test accuracy 86.22926414291791\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b8Nprcd8xM9y"},"source":["\n","def evaluate(model, device, data_loader, criterion, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\n","\tresults = []\n","\n","\tmodel.eval()\n","\n","\twith torch.no_grad():\n","\t\tend = time.time()\n","\t\tfor i, (input, target) in enumerate(data_loader):\n","\t\t\tseqs, lengths = input\n","\t\t\tseqs = seqs.to(device)\n","      \n","\t\t\ttarget = target.to(device)\n","\n","\t\t\toutput = model(seqs, lengths)\n","\t\t\tloss = criterion(output, target)\n","\n","\t\t\t# measure elapsed time\n","\t\t\tbatch_time.update(time.time() - end)\n","\t\t\tend = time.time()\n","\n","\t\t\tlosses.update(loss.item(), target.size(0))\n","\t\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\t\ty_true = target.detach().to('cpu').numpy().tolist()\n","\t\t\ty_pred = output.detach().to('cpu').max(1)[1].numpy().tolist()\n","\t\t\tresults.extend(list(zip(y_true, y_pred)))\n","\n","\t\t\tif i % print_freq == 0:\n","\t\t\t\tprint('Test: [{0}/{1}]\\t'\n","\t\t\t\t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t\t\t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t\t\t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t\t\t\ti, len(data_loader), batch_time=batch_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, results\n","\n","def advEvaluate(model, device, data_loader, criterion, print_freq=10):\n","\tbatch_time = AverageMeter()\n","\tlosses = AverageMeter()\n","\taccuracy = AverageMeter()\n","\teps = 5067*0.01\n","\tresults = []\n","\n","\tmodel.eval()\n","\n","\twith torch.no_grad():\n","\t\tend = time.time()\n","\t\tfor i, (input, target) in enumerate(data_loader):\n","\t\t\tseqs, lengths = input\n","\t\t\tseqs = seqs.to(device) \n","\t\t\tseqs = seqs + eps*(torch.sign(seqs))\n","\t\t\ttarget = target.to(device)\n","\n","\t\t\toutput = model(seqs, lengths)\n","\t\t\tloss = criterion(output, target)\n","\n","\t\t\t# measure elapsed time\n","\t\t\tbatch_time.update(time.time() - end)\n","\t\t\tend = time.time()\n","\n","\t\t\tlosses.update(loss.item(), target.size(0))\n","\t\t\taccuracy.update(compute_batch_accuracy(output, target).item(), target.size(0))\n","\n","\t\t\ty_true = target.detach().to('cpu').numpy().tolist()\n","\t\t\ty_pred = output.detach().to('cpu').max(1)[1].numpy().tolist()\n","\t\t\tresults.extend(list(zip(y_true, y_pred)))\n","\n","\t\t\tif i % print_freq == 0:\n","\t\t\t\tprint('Test: [{0}/{1}]\\t'\n","\t\t\t\t\t  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","\t\t\t\t\t  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","\t\t\t\t\t  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(\n","\t\t\t\t\ti, len(data_loader), batch_time=batch_time, loss=losses, acc=accuracy))\n","\n","\treturn losses.avg, accuracy.avg, results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2Hard7Vi9iG","executionInfo":{"status":"ok","timestamp":1615386859252,"user_tz":-120,"elapsed":7532,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"a4082a99-8495-46e3-89bf-b5891e2cc237"},"source":["test_loss, test_accuracy, test_results = evaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","test_accuracy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0005 (0.0005)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.5152 (0.3427)\tAccuracy 100.000 (86.102)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["86.10165886856657"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07vlCm5-wzZU","executionInfo":{"status":"ok","timestamp":1615408045783,"user_tz":-120,"elapsed":5039522,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"60b914a7-7c3a-4de8-da23-ddc000904e01"},"source":["for i in range(1, 4):\n","  for i in range(1, 5):\n","    model = MyLSTM(num_features)\n","    criterion = nn.CrossEntropyLoss()\n","    NUM_EPOCHS = 20\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_CUDA else \"cpu\")\n","    model.to(device)\n","    criterion.to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0004)\n","    best_val_acc = 0.0\n","    train_losses, train_accuracies = [], []\n","    valid_losses, valid_accuracies = [], []\n","    training_brier = []\n","    t0 = time.time()\n","    for epoch in range(NUM_EPOCHS):\n","\n","      train_loss, train_accuracy, brier = train(model, device, train_loader, criterion, optimizer, epoch, print_freq = len(train_loader)-1)\n","\n","\n","      train_losses.append(train_loss)\n","      training_brier.append(brier)\n","      train_accuracies.append(train_accuracy)\n","    time_one = time.time() - t0\n","    print('NLL Loss is {}'.format(np.mean(train_losses)))\n","    print('Brier score is {}'.format(np.mean(training_brier)))\n","    print('Training time: {} seconds'.format(time_one))\n","    test_loss, test_accuracy, test_results = advEvaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","    print('OOD accuracy {}'.format(test_accuracy))\n","    test_loss, test_accuracy, test_results = evaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","    print('test accuracy {}'.format(test_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["NLL Loss is 0.24228817570935463\n","Brier score is 0.0031832573028838566\n","Training time: 407.3228073120117 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.5987 (0.6799)\tAccuracy 100.000 (78.126)\n","OOD accuracy 78.12632922160783\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0001 (0.0001)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0245 (0.3427)\tAccuracy 100.000 (86.803)\n","test accuracy 86.80348787749894\n","NLL Loss is 0.23818524594632504\n","Brier score is 0.0031351280397109217\n","Training time: 407.1355719566345 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0007 (0.7287)\tAccuracy 100.000 (77.435)\n","OOD accuracy 77.43513398553807\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0001 (0.0001)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0277 (0.3383)\tAccuracy 100.000 (86.857)\n","test accuracy 86.85665674181199\n","NLL Loss is 0.24025462744980125\n","Brier score is 0.003151879648657655\n","Training time: 403.04462695121765 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 2.5238 (0.7680)\tAccuracy 0.000 (74.798)\n","OOD accuracy 74.79795831561037\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0003 (0.0003)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0779 (0.3330)\tAccuracy 100.000 (86.304)\n","test accuracy 86.30370055295619\n","NLL Loss is 0.2408139218921598\n","Brier score is 0.003157407318294402\n","Training time: 407.34723925590515 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 1.7730 (0.7380)\tAccuracy 0.000 (76.701)\n","OOD accuracy 76.70140365801787\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0004 (0.0004)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0347 (0.3283)\tAccuracy 100.000 (86.718)\n","test accuracy 86.71841769459805\n","NLL Loss is 0.24192344174838176\n","Brier score is 0.0031866774732251515\n","Training time: 404.42092061042786 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.7784 (0.6549)\tAccuracy 0.000 (75.670)\n","OOD accuracy 75.66992769034454\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0002 (0.0002)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0441 (0.3369)\tAccuracy 100.000 (86.729)\n","test accuracy 86.72905146746065\n","NLL Loss is 0.24149365680034482\n","Brier score is 0.0031740746272681123\n","Training time: 405.7975969314575 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0005 (0.6761)\tAccuracy 100.000 (79.838)\n","OOD accuracy 79.8383666524883\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0003 (0.0003)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0056 (0.3446)\tAccuracy 100.000 (86.974)\n","test accuracy 86.97362824330072\n","NLL Loss is 0.24193067740058086\n","Brier score is 0.0031819626961495312\n","Training time: 406.50949335098267 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0602 (0.6316)\tAccuracy 100.000 (79.987)\n","OOD accuracy 79.98723947256487\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0002 (0.0002)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0510 (0.3336)\tAccuracy 100.000 (87.133)\n","test accuracy 87.1331348362399\n","NLL Loss is 0.24385161188507715\n","Brier score is 0.003219650805565283\n","Training time: 401.2530438899994 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0018 (0.5789)\tAccuracy 100.000 (81.433)\n","OOD accuracy 81.43343258188006\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0003 (0.0003)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0203 (0.3376)\tAccuracy 100.000 (87.144)\n","test accuracy 87.14376860910251\n","NLL Loss is 0.23925815867253566\n","Brier score is 0.0031506633204323826\n","Training time: 409.29603934288025 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0510 (0.6778)\tAccuracy 100.000 (80.242)\n","OOD accuracy 80.24245002126754\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0001 (0.0001)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0383 (0.3421)\tAccuracy 100.000 (86.963)\n","test accuracy 86.9629944704381\n","NLL Loss is 0.2424158963409313\n","Brier score is 0.0031911333289478773\n","Training time: 407.1843423843384 seconds\n","Test: [0/9404]\tTime 0.003 (0.003)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.2853 (0.6801)\tAccuracy 100.000 (75.808)\n","OOD accuracy 75.80816673755848\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0008 (0.0008)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0866 (0.3327)\tAccuracy 100.000 (86.272)\n","test accuracy 86.27179923436836\n","NLL Loss is 0.2385647291634438\n","Brier score is 0.003134098375783412\n","Training time: 409.65193486213684 seconds\n","Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.2802 (0.7665)\tAccuracy 100.000 (77.467)\n","OOD accuracy 77.4670353041259\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0003 (0.0003)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0968 (0.3440)\tAccuracy 100.000 (85.719)\n","test accuracy 85.71884304551254\n","NLL Loss is 0.24434914704153954\n","Brier score is 0.0032099141586709604\n","Training time: 405.48377561569214 seconds\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0071 (0.6795)\tAccuracy 100.000 (79.700)\n","OOD accuracy 79.70012760527435\n","Test: [0/9404]\tTime 0.001 (0.001)\tLoss 0.0003 (0.0003)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0507 (0.3277)\tAccuracy 100.000 (86.974)\n","test accuracy 86.97362824330072\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bt_8VMGLjAI_","executionInfo":{"status":"ok","timestamp":1615389990133,"user_tz":-120,"elapsed":1568366,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"4b5e9144-90f8-4795-81de-bb1b226ed775"},"source":["test_loss, test_accuracy, test_results = advEvaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","test_accuracy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0000 (0.0000)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0955 (0.6803)\tAccuracy 100.000 (78.754)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["78.75372182050191"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K70Uze7ri_7n","executionInfo":{"status":"ok","timestamp":1615389997209,"user_tz":-120,"elapsed":1574921,"user":{"displayName":"Zhiheng Qian","photoUrl":"","userId":"13654940909956400032"}},"outputId":"1354c086-8df4-41de-e1a6-534fbf7ca287"},"source":["test_loss, test_accuracy, test_results = evaluate(model, device, test_loader, criterion, print_freq = len(test_loader)-1)\n","test_accuracy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test: [0/9404]\tTime 0.002 (0.002)\tLoss 0.0003 (0.0003)\tAccuracy 100.000 (100.000)\n","Test: [9403/9404]\tTime 0.001 (0.001)\tLoss 0.0121 (0.3304)\tAccuracy 100.000 (87.069)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["87.06933219906423"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"JhBuq7Glj4Dl"},"source":[""],"execution_count":null,"outputs":[]}]}